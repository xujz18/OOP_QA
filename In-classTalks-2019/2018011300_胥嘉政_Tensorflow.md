# Tensor Flow
## Introduction
### Purpose
* This  software  design  document  describes  the  core  of  TensorFlow  in  terms  of  itssource  code.   The  analysis  of  the  source  code  will  help  reveal  the  architecture,  thesystem design and the operating principle of TensorFlow.This document is intended for the students and professors who are familiar with theprogramming language of C++ and have great interest in statistical learning.

### Scope
* TensorFlow is an open source code platform which adopts dataflow graph, a spe-cially designed directed acyclic graph, to represent numerical expressions.  More specif-ically,  In  a  dataflow  graph,  the  nodes  ,  which  are  also  referred  to  as  ops,  representabstract  computations,  and  the  edges  represent  the  dataflow  communicated  betweennodes.  Moreover, tensor, after which TensorFlow is named after, is used to representthe data.
* Originally, TensorFlow is developed by researchers and engineers from Google Brainin  order  to  carry  out  the  studies  for  machine  learning  and  depth  neural  networks,including computer vision, voice recognition, natural language processing etc.  However,the flexibility of TensorFlow architecture allows it to be generally applied to numericalcomputations in other fields.

### Overview
* In  this  document,  we’ll  first  describe  the  general  functionality  and  design  of  theTensorFlow system in the part System Overview, followed by the section System Ar-chitecture, where we will talk about the architecture design of the TensorFlow systemin detail.
* Then we’ll explain how the information domain of the TensorFlow system is trans-formed into data structure in the section of Data Design.  After that, we will show theinterface of TensorFlow from the user’s point of view in the section Human InterfaceDesign.
* Finally, we will advance into some of the design patterns and polymorphism featuresin the TensorFlow project, together with various skills we’ve learned through the processof code reading.

### Reference Material
	1.  Liu Guangcong.  TensorFlow Internals.
	2.  https://blog.csdn.net/qq20657717/article/details/82902868
	3.  http://www.tensorfly.cn/
	4.https://blog.csdn.net/weixin38611497/article/details/82053104
	5.https://www.cnblogs.com/shouhuxianjian/p/9107539.html
	6.https://www.cnblogs.com/tengge/p/6360946.html
	7.https://blog.csdn.net/qq34580082/article/details/79896313
	
## System Overview
* The overall system of TensorFlow is subtly designed and well organized.  The systemis separated by C API into two parts:  frontend and backend respectively.  Users canwrite their codes in the frontend with various types of programming languages, and withthe connection by C API, the backend will offer the runtime environment, including theconstruction and execution of the dataflow graph corresponding to users’ codes. 
* TensorFlow is an open source project.  The complete source code of TensorFlow canbe cloned from Github with the command:
```
git clone git@github.com:tensorflow/tensorflow.git
```
* To compile and run TensorFlow from its source code, users have to install Bazel,a  construction  tool  which  TensorFlow’s  developers  adopted,  to  compile  TensorFlowsource code.  The command to compile the source code on Linux is as follows:


```
bazel build --config=opt
//tensorflow/tools/pip_package:build_pip_package
```

* When GPU is available, we can add the compile option ’–config=cuda’.  In this case,one should be familiar with the version of CUDA and cuDNN on his or her computer,for such information will be queried during the process of compiling.


```
bazel build -c opt --config=cuda \2//tensorflow/tools
pip_package:build_pip_package
```

## System Architecture
### Architecture Design
The design of TensorFlow architecture follows several basic principles to guide theimplementation of the whole system.  Here are some of the principles:

1.  Computation Delay:  The construction and execution of graph are separated andthe execution is delayed to reduce the time consumption of real-time execution.
2.  Atom op: op is the most basic abstract computation unit to construct the complexnetwork models.
3.  Abstraction of Tasks:  The parameter server is based on abstract tasks, which isflexible and extendable to new optimization algorithms and models.
4.  Abstraction of Devices:  CPU, GPU, ASIC etc.  All these computational deviceswith different architectures are supported with the abstraction of devices.


First of all, we construct a data flow graph, and then our data which exist as the formof tensor were put into the data graph to be calculated.  In the graph, nodes representmathematical operations while edges represented multidimensional arrays interrelatedwith  each  other  between  nodes.   When  the  model  is  being  trained,  tensors  will  flowfrom a node to another in the data graph.Session is command for Tensorflow to control and output files. Running session.run()can get calculate results you want or the part you are going to calculate.The process of whole work of Tensowflow can be mainly divided into four parts:

1.  Client:  Client will transfer whole calculating process into a data flow graph whichwill be passed through session to master to operate.
2.  Distributed Master:  Based on parameters passed by clients to the session.run(),distributed  master  will  trim  the  complete  graph  and  extract  subgraphs  whichwill be divided into different parts to enter corresponding processes and devisesand then distributed to different worker services.  After receiving graph sections,worker services will begin to operate.
3.  Worker Services:  They use free kernel such as CPU or GPU to plan to implementcalculation parts of graph sections which have been received.  During planning,worker services send and receive calculation results with each other.
4.  Kernel Implementations:  In this part, calculation parts of graph operations willbe implemented.

## Data Design
### Data Description
In the dataflow graph, most of the data is stored in the form of edges and nodes,which forms the whole graph.The  precursor  node  and  successor  node  are  stored  in  each  edge  to  maintain  theconnectivity of the graph.  In the graph, each node can have multiple input edges oroutput edges.A normal edge can store the data describing the relation (tensor) between the nodes.(In general, it is the relation between the nodes that generate the information and thenodes that receive the information.  )  Some special edges only describe the dependencebetween the nodes when they are executed.The inedges and outedges are stored in the nodes for routing.  And of course anode also holds the information about the device allocation and the operator.The graph for computation is a DAG compose by edges and nodes.  When the usermakes a request, the graph will execute the operations in the topological order.
### Data Dictionary
Here is a list of the fundamental classes with more detailed information.  (Since thereare too many data types, we only picked some important classes in the ’graph’ part.  )

* Edge

```
Features:
Node  src:  the pointer pointing to the precursor node.
Node  dst:  the pointer pointing to the successor node.
int id :  the number to identify the edge.
int srcoutput:  the number of the source output that produces the data carried bythis edge.
int dstinput:  the number of the destination input that consumes the data carriedby this edge.
```
```
Methods:
Node src() const :  return src.
Node dst() const :  return dst.
int id() const :  return the value of id.
int srcoutput() const :  return the value of srcoutput.
int dstoutput() const :  return the value of dstoutput.
bool IsControlEdge() const :  return true iff this is an edge that indicates a control-flow.
```

* Node

```
Features:
enum NodeClass :  A set of mutually exclusive classes for different kinds of nodes.
NodeClass class:  Type of the node.int id:  the unique number assigned to the node, -1 until Initialize() is called.
int costid :  Corresponding cost to the node.EdgeSet inedges:  Edges that points to the node.
EdgeSet outedges:  Edges that begins from the node.
int assigneddevicenameindex:  Index within Graph::devicenamesof the nameof device assigned to perform this computation.
Graph* graph:  A back-pointer to the Graph that owns this node.
``` 
```
Methods:
string DebugString() const :int id() const :  return the value of id.
int costid() const :  return the value of costid.
const string& requesteddevice() const :  The device requested by the user.
int assigneddevicenameindex() const :  return assigneddevicenameindex.
bool is’X’() :  return whether the type of the node is ’X’.
void setrequesteddevice(const string& device) :  This changes the user requesteddevice but not necessarily the device that on which the operation will run.
const NodeDef& def() const :  def() provides the NodeDef the user supplied, but thespecifics of this Node may have changed due to placement, optimization, etc.
```

* Graph

```
Features:
std::vector<Node*>nodes: Map from node ids to allocated nodes.  nodes[id] maybe nullptr if the node with that id was removed from the graph.
int64 numnodes:  Number of nodes.
std::vector<Edge*>edges:  Map from edge ids to allocated edges.  edges[id] maybe nullptr if the edge with that id was removed from the graph.
int numedges:  The number of entries in edgesthat are not nullptr.
std::vector<Node*>freenodes :  Allocated but free nodes.
std::vector<Edge*>freeedges :  Allocated but free edges.
int namecounter:  Generating unique names.
```
```
Methods:
explicit  Graph(const  OpRegistryInterface*  registry)  :   Constructs  a  graph  witha  single  SOURCE  and  a  single  SINK  (always  id  kSinkId)  node,  and  an  edge  fromSOURCE−>SINK.  The  graph  can  hold  ops  found  in  registry.   ’registry’  ’s  lifetimemust be at least that of the constructed graph’s.
Edge* FindEdge(const Node* dst, int index) : Searches through edgesfor the Edgewhose destination node and index matches dst.  An edge with destination ’dst’ mustexist in the graph.
explicit  Graph(const  FunctionLibraryDefinition&  flibdef)  :   Constructs  a  graphwith a single SOURCE (always id kSourceId) and a single SINK (always id kSinkId)node, and an edge from SOURCE->SINK. The graph can hold ops found in ’flibdef’.Unlike the constructor taking an OpRegistryInterface, this constructor copies the func-tion definitions in ’flibdef’ so its lifetime may be shorter than that of the graph’s.  TheOpRegistryInterface backing ’flibdef’ must still have the lifetime of the graph though.
 ̃Graph() :Node* AddNode(const NodeDef& nodedef, Status* status) :  Adds a new node tothis graph and returns it.
 Node* CopyNode(Node* node) :  Copies node, which may belong to another graphto a new node, which is returned.  It Does not copy any edges.
 Edge* AddEdge(Node* source, int x, Node* dest, int y) : Add an edge that connectsthe xth output of ”source” to the yth input of ”dest”.
 Edge* AddControlEdge(Node* source, Node* dest) :  Add a control-edge (no dataflows along this edge) that connects ”source” to ”dest”.
```

## Human Interface Design

### Overview of User Interface
Since TensorFlow is most easily used with python and some of the python librariesfor  TensorFlow  is  quite  popular,  we  will  mainly  focus  on  the  interface  designed  forpython language.  Here are some of the basic interfaces to support several applicationsof TensorFlow.

#### Graph Construction
The first step to construct a graph is to create ’source op’.  Such ops don’t requireinput data, instead they can be passed to other ops as the input.  With the assistance offunctions ’constant()’, ’getvariable()’ or ’placeholder()’, source op can be easily created.We now show the signature of these functions.The function ’constant()’ which is used to  create  constants  during  the process ofcomputing:

```
def constant(value, dtype=None, shape=None, name="Const"):
```

The function ’getvariable()’ which is used to create a variable from an existing oneor a new one (More specifically, if a variable with the given name is already stored, thestored variable will be returned.  Otherwise, a new one will be created.):

```
1	def get_variable(self,  name,  shape=None, dtype=dtypes.float32,
2	initializer=None, regularizer=None, reuse=None,
3	trainable=None, collections=None, caching_device=None,
4	partitioner=None, validate_shape=True, use_resource=None,
5	custom_getter=None, constraint=None
6	synchronization=VariableSynchronization.AUTO
7	aggregation=VariableAggregation.NONE):
```
#### Graph Execution
To run a graph, a session must be created with the constructor of the Session class,whose signature is:

```
def __init__(self, target='', graph=None, config=None):
```

If there’s no parameter given when calling the Session constructor, the session con-structed will use the default graph for execution.  To run the operations in the graphof the session, member function ’run’ is helpful, whose signature is:

```
def run(self, fetches, feed_dict=None, options=None, run_metadata=None):
```

When the function is called, the graph will be executed and the tensors in the pa-rameter ’fetches’ will be evaluated and returned.  And the optional parameter feeddictallows the caller to override the value of tensors in the graph.

After the session is over, users have to close it to release all the resources occupiedby the session by either explicitly using the member function ’close()’ or using a ’with’block in python.

#### Screen Images
Here is a simple example to display the usage of the basic interface:

```
1	import tensorflow as tf
2	
3	#add source ops
4	mat_a = tf.Variable([[3,3],[1,2]])
5	
6	mat_b = tf.Variable([[1,1],[2,1]])
7	
8	#initialize all the variables
9	init = tf.global_variables_initializer()
10	
11	#add an op for matrix production
12	mat_prod = tf.matmul(mat_a, mat_b)
13
14	#Open a session to run the graph
15	with tf.Session() as sess:
16	sess.run(init)
17	result = sess.run([mat_prod])
18	print(result)
```

In this example, two variables called ’mata’ and ’matb’ are created and added assource ops into the default graph.  An maths op of ’matmul’ to calculate the productbetween two matrices are also added to the graph.  Because those tensors involved inthe graph are all variables, they have to be initialized before executed in a graph.  So aninitializer op should be also added to the graph and run first in the session.  After theinitialization, the graph is executed and the result of the matrix production is evaluatedand printed.

## Design Patterns and Polymorphism Features

Several classical design patterns and polymorphism features have been adopted inthe  system  and  architecture  design  of  TensorFlow.   One  of  the  applications  of  thosepatterns and features is the design of the TensorFlow memory allocator.  As is knownto all,  different ops may be performed on different devices,  and sometimes memorieson  GPU  have  to  be  allocated  while  sometimes  it’s  the  memories  on  the  CPU  RAMthat are required.  So the allocator must show polymorphism features to meet diffierentrequirements on different conditions.

To implement this, a base class called Allocator is created with several pure virtualfunctions, here are the signatures of these virtual functions:

    //Return a string identifying this allocator
    virtual string Name() = 0;
 
    // Return an uninitialized block of memory that is "num_bytes" bytes
    // in size.  The returned pointer is guaranteed to be aligned to a
    // multiple of "alignment" bytes.
    // REQUIRES: "alignment" is a power of 2.
    virtual void* AllocateRaw(size_t alignment, size_t num_bytes) = 0;
  
    // Return an uninitialized block of memory that is "num_bytes" bytes
    // in size with specified allocation attributes.  
    // The returned pointer is
    // guaranteed to be aligned to a multiple of "alignment" bytes.
    // REQUIRES: "alignment" is a power of 2.
    virtual void* AllocateRaw(size_t alignment, size_t num_bytes,
                           const AllocationAttributes& allocation_attr) {
      // The default behavior is to use the implementation without
      // any allocation attributes.
      return AllocateRaw(alignment, num_bytes);
    }
  
    // Deallocate a block of memory pointer to by "ptr"
    // REQUIRES: "ptr" was previously returned by a call to AllocateRaw
    virtual void DeallocateRaw(void* ptr) = 0;

Users can implement their own version of memory allocators derived from the baseAllocator,  and  the  pointer  of  Allocator  pointing  to  a  derived  allocator  will  show  apolymorphism feature, which is a typical kind of template method design pattern.

Apart from template method, a derived class of Allocator called AllocatorWrapperhave  been  designed.   The  class  AllocatorWrapper  is  an  implementation  of  Allocatorthat delegates all calls to another Allocator.  Part of the implementation is showed asfollows:

	class AllocatorWrapper : public Allocator {
	 public:
	  explicit AllocatorWrapper(Allocator* wrapped) : wrapped_(wrapped) {}
	 
	  ~AllocatorWrapper() override {}
	 
	  // Returns the wrapped allocator to which all calls are delegated.
	  Allocator* wrapped() const { return wrapped_; }
		 
	  string Name() override { return wrapped_->Name(); }
	 
	  void* AllocateRaw(size_t alignment, size_t num_bytes) override {
	    return wrapped_->AllocateRaw(alignment, num_bytes);
	  }
	 
	  void* AllocateRaw(size_t alignment, size_t num_bytes,
	              const AllocationAttributes& allocation_attr) override {
	     return wrapped_->AllocateRaw(alignment, num_bytes, allocation_attr);
	  }
	 
	  void DeallocateRaw(void* ptr) override { wrapped_->DeallocateRaw(ptr); }
	  ...

Users can inherit this class and add his own controls or methods to this wrapper class, which is useful to the ones who want to override part of the functionality of another memory allocator. This design can be viewed as a kind of proxy design pattern.

Moreover, abstract factory pattern have been used to easily build different types of memory allocators. An abstract class called AllocatorFactory have been designed with several pure virtual functions. Here is the declaration of the class:

 	 class AllocatorFactory {
   	public:
   	 virtual ~AllocatorFactory() {}
  
    // Returns true if the factory will create a functionally different
    // SubAllocator for different (legal) values of numa_node.
    virtual bool NumaEnabled() { return false; }
  
    // Create an Allocator.
    virtual Allocator* CreateAllocator() = 0;
  
    // Create a SubAllocator. If NumaEnabled() is true, 
    // then returned SubAllocator
    // will allocate memory local to numa_node. 
    // If numa_node == kNUMANoAffinity
    // then allocated memory is not specific to any NUMA node.
    virtual SubAllocator* CreateSubAllocator(int numa_node) = 0;
  	};

